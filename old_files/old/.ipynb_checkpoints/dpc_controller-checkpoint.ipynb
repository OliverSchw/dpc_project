{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1919a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import equinox as eqx\n",
    "from helpers import random_initial_state\n",
    "\n",
    "import exciting_environments as excenvs\n",
    "\n",
    "@dataclass\n",
    "class EnvParameters:\n",
    "    name: str\n",
    "    batch_size: int\n",
    "    l: float\n",
    "    m: float\n",
    "    tau: float\n",
    "    max_torque: float\n",
    "\n",
    "@dataclass\n",
    "class DPCParameters:\n",
    "    layer_sizes: list\n",
    "    learning_rate: float\n",
    "    optimizer_type: str\n",
    "    epochs: int\n",
    "\n",
    "@dataclass\n",
    "class TrainingResults:\n",
    "    losses: list\n",
    "    state_trajectories: list\n",
    "    action_trajectories: list\n",
    "\n",
    "def train_dpc_controller(env_params, dpc_params, key):\n",
    "    env = excenvs.make(env_params.name, batch_size=env_params.batch_size, l=env_params.l, m=env_params.m, tau=env_params.tau, max_torque=env_params.max_torque)\n",
    "\n",
    "    class PolicyNetwork(eqx.Module):\n",
    "        layers: list[eqx.nn.Linear]\n",
    "        \n",
    "        def __init__(self, layer_sizes, key):\n",
    "            self.layers = []\n",
    "            for (fan_in, fan_out) in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "                key, subkey = jax.random.split(key)\n",
    "                self.layers.append(eqx.nn.Linear(fan_in, fan_out, use_bias=True, key=subkey))\n",
    "        \n",
    "        def __call__(self, x):\n",
    "            for layer in self.layers[:-1]:\n",
    "                x = jax.nn.tanh(layer(x))\n",
    "            return jnp.tanh(self.layers[-1](x))\n",
    "\n",
    "    ref_state = jnp.tile(jnp.array([[0, 0]]), (env_params.batch_size, 1)).astype(jnp.float32)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def loss_fn(policy, initial_state, ref_state, key):\n",
    "        def generate_actions(carry, _):\n",
    "            state, key = carry\n",
    "            key, subkey = jax.random.split(key)\n",
    "            policy_params = jnp.concatenate([state, ref_state], axis=-1)\n",
    "            action = jax.vmap(policy)(policy_params)\n",
    "            next_state = jax.vmap(env._ode_exp_euler_step)(state, action, env.env_state_normalizer, env.action_normalizer, env.static_params)\n",
    "            return (next_state, key), (next_state, action, state)\n",
    "        (_, (predict_states, actions, initial_states)) = jax.lax.scan(generate_actions, (initial_state, key), None, length=3000)\n",
    "        mse = jnp.mean((predict_states - ref_state)**2)\n",
    "        return mse, predict_states, actions, initial_states\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def compute_loss(policy, initial_state, key):\n",
    "        mse_loss, _, _, _ = loss_fn(policy, initial_state, ref_state, key)\n",
    "        return mse_loss\n",
    "\n",
    "    if dpc_params.optimizer_type == 'adam':\n",
    "        optimizer = optax.adam(dpc_params.learning_rate)\n",
    "    elif dpc_params.optimizer_type == 'rmsprop':\n",
    "        optimizer = optax.rmsprop(dpc_params.learning_rate, decay=0.9, eps=1e-8)\n",
    "    elif dpc_params.optimizer_type == 'sgd':\n",
    "        optimizer = optax.sgd(dpc_params.learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer type: {dpc_params.optimizer_type}\")\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    policy = PolicyNetwork(dpc_params.layer_sizes, key=key)\n",
    "    opt_state = optimizer.init(policy)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def update_state(policy, initial_state, key, opt_state):\n",
    "        loss, grads = compute_loss(policy, initial_state, key)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        policy = eqx.apply_updates(policy, updates)\n",
    "        return loss, policy, opt_state\n",
    "\n",
    "    losses = []\n",
    "    state_trajectories = []\n",
    "    action_trajectories = []\n",
    "\n",
    "    for epoch in range(dpc_params.epochs):\n",
    "        jax_key, subkey = jax.random.split(key)\n",
    "        batch_initial_states = random_initial_state(subkey, env_params.batch_size)\n",
    "        loss, policy, opt_state = update_state(policy, batch_initial_states, subkey, opt_state)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        _, predicted_states, actions, initial_states = loss_fn(policy, batch_initial_states, ref_state, subkey)\n",
    "        state_trajectories.append(predicted_states)\n",
    "        action_trajectories.append(actions)\n",
    "\n",
    "    results = TrainingResults(losses=losses, state_trajectories=state_trajectories, action_trajectories=action_trajectories)\n",
    "    return policy, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b17b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import exciting_environments as excenvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92cf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = excenvs.make('Pendulum-v0', batch_size=1, l=1, m=1, tau=1e-2, max_torque=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c248eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from helpers import random_initial_state, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98110823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
